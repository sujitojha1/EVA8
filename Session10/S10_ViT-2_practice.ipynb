{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='./data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(*layers)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.gamma * self.residual(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormChannels(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, -1)\n",
    "        x = self.norm(x)\n",
    "        x = x.transpose(-1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, head_channels, shape):\n",
    "        super().__init__()\n",
    "        self.heads = out_channels // head_channels\n",
    "        self.head_channels = head_channels\n",
    "        self.scale = head_channels**-0.5\n",
    "        \n",
    "        self.to_keys = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.to_queries = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.to_values = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.unifyheads = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        \n",
    "        height, width = shape\n",
    "        self.pos_enc = nn.Parameter(torch.Tensor(self.heads, (2 * height - 1) * (2 * width - 1)))\n",
    "        self.register_buffer(\"relative_indices\", self.get_indices(height, width))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        \n",
    "        keys = self.to_keys(x).view(b, self.heads, self.head_channels, -1)\n",
    "        values = self.to_values(x).view(b, self.heads, self.head_channels, -1)\n",
    "        queries = self.to_queries(x).view(b, self.heads, self.head_channels, -1)\n",
    "        \n",
    "        att = keys.transpose(-2, -1) @ queries\n",
    "        \n",
    "        indices = self.relative_indices.expand(self.heads, -1)\n",
    "        rel_pos_enc = self.pos_enc.gather(-1, indices)\n",
    "        rel_pos_enc = rel_pos_enc.unflatten(-1, (h * w, h * w))\n",
    "        \n",
    "        att = att * self.scale + rel_pos_enc\n",
    "        att = F.softmax(att, dim=-2)\n",
    "        \n",
    "        out = values @ att\n",
    "        out = out.view(b, -1, h, w)\n",
    "        out = self.unifyheads(out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_indices(h, w):\n",
    "        y = torch.arange(h, dtype=torch.long)\n",
    "        x = torch.arange(w, dtype=torch.long)\n",
    "        \n",
    "        y1, x1, y2, x2 = torch.meshgrid(y, x, y, x, indexing='ij')\n",
    "        indices = (y1 - y2 + h - 1) * (2 * w - 1) + x1 - x2 + w - 1\n",
    "        indices = indices.flatten()\n",
    "        \n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, mult=4):\n",
    "        hidden_channels = in_channels * mult\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, hidden_channels, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_channels, out_channels, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Sequential):\n",
    "    def __init__(self, channels, head_channels, shape, p_drop=0.):\n",
    "        super.__init__(\n",
    "            Residual(\n",
    "                LayerNormChannels(channels),\n",
    "                SelfAttention2d(channels, channels, head_channels, shape),\n",
    "                nn.Dropout(p_drop)\n",
    "            ),\n",
    "            Residual(\n",
    "                LayerNormChannels(channels),\n",
    "                FeedForward(channels,channels),\n",
    "                nn.Dropout(p_drop)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStack(nn.Sequential):\n",
    "    def __init__(self, numb_blocks, channels, head_channels, shape, p_drop=0.):\n",
    "        layers = [TransformerBlock(channels, head_channels, shape, p_drop) for _ in range(numb_blocks)]\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToPatches(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels, patch_size, hidden_channels = 32):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, hidden_channels, 3, padding = 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_channels, channels, patch_size, stride=patch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionEmbedding(nn.Module):\n",
    "    def __init__(self, channels, shape):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.Tensor(channels, *shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToEmbedding(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels, patch_size, shape, p_drop=0.):\n",
    "        super().__init__(\n",
    "            ToPatches(in_channels, channels, patch_size),\n",
    "            AddPositionEmbedding(channels, shape),\n",
    "            nn.Dropout(p_drop)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Sequential):\n",
    "    def __init__(self, in_channels, classes, p_drop=0.):\n",
    "        super().__init__(\n",
    "            LayerNormChannels(in_channels),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(in_channels, classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, classes, image_size, channels, head_channels, num_blocks, patch_size, \n",
    "                 in_channels=3, emb_p_drop=0., trans_p_drop=0., head_p_drop=0.):\n",
    "        reduced_size = image_size // patch_size\n",
    "        shape = (reduced_size, reduced_size)\n",
    "        \n",
    "        super().__init__(\n",
    "            ToEmbedding(in_channels, channels, patch_size, shape, emb_p_drop),\n",
    "            TransformerStack(num_blocks, channels, head_channels, shape, trans_p_drop),\n",
    "            Head(channels, classes, head_p_drop)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, AddPositionEmbedding):\n",
    "                nn.init.normal_(m.pos_embedding, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, SelfAttention2d):\n",
    "                nn.init.normal_(m.pos_enc, mean=0.0, std=0.02)\n",
    "            elif isinstance(m, Residual):\n",
    "                nn.init.zeros_(m.gamma)\n",
    "\n",
    "    def separate_parameters(self):\n",
    "        parameters_decay = set()\n",
    "        parameters_no_decay = set()\n",
    "        modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
    "        modules_no_weight_decay = (nn.LayerNorm,)\n",
    "\n",
    "        for m_name, m in self.named_modules():\n",
    "            for param_name, param in m.named_parameters():\n",
    "                full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
    "\n",
    "                if isinstance(m, modules_no_weight_decay):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif param_name.endswith(\"bias\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, Residual) and param_name.endswith(\"gamma\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, AddPositionEmbedding) and param_name.endswith(\"pos_embedding\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, SelfAttention2d) and param_name.endswith(\"pos_enc\"):\n",
    "                    parameters_no_decay.add(full_param_name)\n",
    "                elif isinstance(m, modules_weight_decay):\n",
    "                    parameters_decay.add(full_param_name)\n",
    "\n",
    "        # sanity check\n",
    "        # assert len(parameters_decay & parameters_no_decay) == 0\n",
    "        # assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
    "\n",
    "        return parameters_decay, parameters_no_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES, IMAGE_SIZE = 10, 32\n",
    "model = ViT(NUM_CLASSES, IMAGE_SIZE, channels = 32, head_channels=8, num_blocks=4, patch_size=2, \n",
    "            emb_p_drop=0., trans_p_drop=0., head_p_drop=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
