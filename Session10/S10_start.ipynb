{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "  def __init__(self, fn):\n",
    "    super().__init__()\n",
    "    self.fn = fn \n",
    "  def forward(self, x):\n",
    "    return self.fn(x) + x\n",
    "\n",
    "\n",
    "def ConvMixer(dim, depth, kernel_size=5, patch_size=2, n_classes=10):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "      nn.GELU(),\n",
    "      nn.BatchNorm2d(dim),\n",
    "      *[nn.Sequential(\n",
    "          Residual(nn.Sequential(\n",
    "              nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "              nn.GELU(),\n",
    "              nn.BatchNorm2d(dim)\n",
    "          )),\n",
    "          nn.Conv2d(dim, dim, kernel_size=1),\n",
    "          nn.GELU(),\n",
    "          nn.BatchNorm2d(dim)\n",
    "      ) for i in range(depth)],\n",
    "      nn.AdaptiveAvgPool2d((1, 1)),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(dim, n_classes)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:29, 5701256.34it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "])\n",
    "\n",
    "epochs = 25\n",
    "batch_size =512\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvMixer: Epoch: 0 | Train Acc: 0.3446, Test Acc: 0.5184, Time: 41.9, lr: 0.001000\n",
      "ConvMixer: Epoch: 1 | Train Acc: 0.5488, Test Acc: 0.6067, Time: 36.2, lr: 0.002000\n",
      "ConvMixer: Epoch: 2 | Train Acc: 0.6404, Test Acc: 0.6552, Time: 35.3, lr: 0.003000\n",
      "ConvMixer: Epoch: 3 | Train Acc: 0.7036, Test Acc: 0.7118, Time: 35.7, lr: 0.004000\n",
      "ConvMixer: Epoch: 4 | Train Acc: 0.7402, Test Acc: 0.7787, Time: 36.6, lr: 0.005000\n",
      "ConvMixer: Epoch: 5 | Train Acc: 0.7622, Test Acc: 0.7923, Time: 36.5, lr: 0.006000\n",
      "ConvMixer: Epoch: 6 | Train Acc: 0.7822, Test Acc: 0.7995, Time: 36.1, lr: 0.007000\n",
      "ConvMixer: Epoch: 7 | Train Acc: 0.7944, Test Acc: 0.7753, Time: 36.6, lr: 0.008000\n",
      "ConvMixer: Epoch: 8 | Train Acc: 0.8044, Test Acc: 0.7953, Time: 36.3, lr: 0.009000\n",
      "ConvMixer: Epoch: 9 | Train Acc: 0.8163, Test Acc: 0.8191, Time: 36.6, lr: 0.010000\n",
      "ConvMixer: Epoch: 10 | Train Acc: 0.8272, Test Acc: 0.8437, Time: 34.9, lr: 0.009050\n",
      "ConvMixer: Epoch: 11 | Train Acc: 0.8447, Test Acc: 0.8513, Time: 34.9, lr: 0.008100\n",
      "ConvMixer: Epoch: 12 | Train Acc: 0.8579, Test Acc: 0.8673, Time: 34.9, lr: 0.007150\n",
      "ConvMixer: Epoch: 13 | Train Acc: 0.8698, Test Acc: 0.8682, Time: 34.8, lr: 0.006200\n",
      "ConvMixer: Epoch: 14 | Train Acc: 0.8818, Test Acc: 0.8771, Time: 34.9, lr: 0.005250\n",
      "ConvMixer: Epoch: 15 | Train Acc: 0.8900, Test Acc: 0.8938, Time: 35.0, lr: 0.004300\n",
      "ConvMixer: Epoch: 16 | Train Acc: 0.8999, Test Acc: 0.8952, Time: 35.6, lr: 0.003350\n",
      "ConvMixer: Epoch: 17 | Train Acc: 0.9101, Test Acc: 0.9016, Time: 35.3, lr: 0.002400\n",
      "ConvMixer: Epoch: 18 | Train Acc: 0.9203, Test Acc: 0.9100, Time: 35.2, lr: 0.001450\n",
      "ConvMixer: Epoch: 19 | Train Acc: 0.9293, Test Acc: 0.9137, Time: 36.1, lr: 0.000500\n",
      "ConvMixer: Epoch: 20 | Train Acc: 0.9351, Test Acc: 0.9141, Time: 36.3, lr: 0.000400\n",
      "ConvMixer: Epoch: 21 | Train Acc: 0.9361, Test Acc: 0.9169, Time: 35.3, lr: 0.000300\n",
      "ConvMixer: Epoch: 22 | Train Acc: 0.9381, Test Acc: 0.9164, Time: 36.3, lr: 0.000200\n",
      "ConvMixer: Epoch: 23 | Train Acc: 0.9397, Test Acc: 0.9165, Time: 35.8, lr: 0.000100\n",
      "ConvMixer: Epoch: 24 | Train Acc: 0.9412, Test Acc: 0.9171, Time: 35.0, lr: 0.000000\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = lambda t: np.interp([t], [0, epochs*2//5, epochs*4//5, epochs], \n",
    "                                  [0, 0.01, 0.01/20.0, 0])[0]\n",
    "\n",
    "depth = 10\n",
    "hdim = 256\n",
    "psize = 2\n",
    "conv_ks = 5\n",
    "clip_norm = True\n",
    "\n",
    "model = ConvMixer(hdim, depth, patch_size=psize, kernel_size=conv_ks, n_classes=10)\n",
    "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss, train_acc, n = 0, 0, 0\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        model.train()\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "\n",
    "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
    "        opt.param_groups[0].update(lr=lr)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if clip_norm:\n",
    "            scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item() * y.size(0)\n",
    "        train_acc += (output.max(1)[1] == y).sum().item()\n",
    "        n += y.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    test_acc, m = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(X)\n",
    "            test_acc += (output.max(1)[1] == y).sum().item()\n",
    "            m += y.size(0)\n",
    "\n",
    "    print(f'ConvMixer: Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
