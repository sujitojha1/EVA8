{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "def ConvMixer(dim, depth, kernel_size=5, patch_size=2, n_classes=10):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "            Residual(nn.Sequential(\n",
    "                nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "            )),\n",
    "            nn.Conv2d(dim, dim, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(dim)\n",
    "        ) for i in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d(1,1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio = (1.0, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "])\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 512\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=trains_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='.data', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = lambda t: np.interp([t], [0, epochs*2//5, epochs*4//5, epochs],\n",
    "                                    [0,0.01,0.01/20.0,0])[0]\n",
    "\n",
    "depth = 10\n",
    "hdim = 256\n",
    "psize = 2\n",
    "conv_ks = 5\n",
    "clip_norm = True\n",
    "\n",
    "model = ConvMixer(hdim, depth, patch_size =psize, kernel_size=conv_ks, n_classes=10)\n",
    "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss, train_acc, n = 0, 0, 0\n",
    "    for i, (X,y) in enumerate(trainloader):\n",
    "        model.train()\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "\n",
    "        lr = lr_scheduler(epoch + (i +1)/len(trainloader))\n",
    "        opt.param_groups[0].update(lr=lr)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(X)\n",
    "            loss = criterion(output, x)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if clip_norm:\n",
    "            scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        \n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "        train_acc += (output.max(1)[1] == y).sum().item()\n",
    "        n += y.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    test_acc, m = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X,y) in enumerate(testloader):\n",
    "            X,y = X.cuda(), y.cuda()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(X)\n",
    "\n",
    "            test_acc += (output.max(1)[1] == y).sum().item()\n",
    "            m += y.size(0)\n",
    "\n",
    "    print(f'ConvMixer: Epoch: {epoch} | Train_acc/n:.4f, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
